\documentclass[a4paper,twoside]{article}

\usepackage{epsfig}
%\usepackage{subfigure}
\usepackage{calc}
\usepackage{amssymb}
\usepackage{amstext}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{acronym}
\usepackage{graphicx}
\usepackage{color}
\usepackage{url}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{bigstrut}
\usepackage{rotating}
\usepackage{pslatex}
\usepackage{apalike}
\usepackage{SCITEPRESS}     % Please add other packages that you may need BEFORE the SCITEPRESS.sty package.

%\subfigtopskip=0pt
%\subfigcapskip=0pt
%\subfigbottomskip=0pt


\newlength{\frameSize}

% --| My Environments |---------------------------------------------------
\newcommand{\vertboxl}[1]{\rotatebox{90}{\parbox{21mm}{\centering #1}}}
\newcommand{\vertboxm}[1]{\rotatebox{90}{\parbox{17mm}{\centering #1}}}
\newcommand{\vertboxs}[1]{\rotatebox{90}{\parbox{14mm}{\centering #1}}}
\newcommand{\vertboxt}[1]{\rotatebox{90}{\parbox{10mm}{\centering #1}}}
\newcommand{\vertboxxt}[1]{\rotatebox{90}{\parbox{7mm}{\centering #1}}}
\newcommand{\sizebox}[2]{\parbox{#1}{\centering #2}}

% --| My Acronyms |---------------------------------------------------
\newacro{CNN}{Convolutional Neural Network}
\newacro{CRF}{Conditional Random Field}
\newacro{FCN}{Fully Convolutional Neural Network}
\newacro{FROC}{Free-Response Receiver Operating Characteristic}
\newacro{GMM}{Gaussian Mixture Model}
\newacro{HOG}{Histogram of Oriented Gradients}
\newacro{LitW}{Logos in the Wild}
\newacro{PCA}{Principle Component Analysis}
\newacro{SIFT}{Scale Invariant Feature Transform}
\newacro{SVM}{Support Vector Machine}

% --| My Macros |-------------------------------------------------------
\newcommand{\upto}{\, ,...\, ,\,}	% ,...,
\newcommand{\fuerdiegilt}{\;|\;}
\newcommand{\etal}{et\,al.}
\newcommand{\xie}{i.e.}
\newcommand{\xeg}{e.g.}
\newcommand{\newR}{\ensuremath{\mathbb{R}}}
\newcommand{\cross}{$\,\times\,$}
\newcommand{\tightcross}{$\times$}
\newcommand{\tighttimes}{{\mkern-1mu\times\mkern-1mu}}
\newcommand{\ve}[1]{\boldsymbol{#1}}
\newcommand{\veb}[1]{\boldsymbol{\overline{#1}}}
\newcommand{\veh}[1]{\boldsymbol{\widehat{#1}}}
\newcommand{\vet}[1]{\widetilde{\boldsymbol{#1}}}
\newcommand{\mc}[1]{\mathcal{#1}}
\newcommand{\landau}[1]{$\mathcal{O}\left(#1\right)$}
\newcommand{\sign}[1]{\mathrm{sign}(#1)}

% --| Actual variables |-----------------------------------------------
\newcommand{\mne}{\mathit{MNE}}
\newcommand{\map}{$\mathit{map}$}
\newcommand{\acc}{\mathit{acc}}
\newcommand{\std}{$\mathit{std}$}
\newcommand{\TP}{\mathit{TP}}
\newcommand{\TN}{\mathit{TN}}
\newcommand{\FP}{\mathit{FP}}
\newcommand{\FN}{\mathit{FN}}
\newcommand{\TPR}{\mathit{TPR}}
\newcommand{\TNR}{\mathit{TNR}}
\newcommand{\FPR}{\mathit{FPR}}
\newcommand{\FNR}{\mathit{FNR}}



\begin{document}

\title{Open Set Logo Detection and Retrieval}

\author{\authorname{Andras T\"uzk\"o\sup{1}, Christian Herrmann\sup{1,2}, Daniel Manger\sup{1}, Dieter Willersinn\sup{1}, J\"urgen Beyerer\sup{1,2}}
\affiliation{\sup{1} Fraunhofer IOSB, Karlsruhe, Germany}
\affiliation{\sup{2} Karlsruhe Institute of Technology KIT, Vision and Fusion Lab, Karlsruhe, Germany}
\email{$\lbrace$andras.tuezkoe$|$christian.herrmann$|$daniel.manger$|$dieter.willersinn$\rbrace$@iosb.fraunhofer.de}
%\email{\{f\_author, s\_author\}@ips.xyz.edu, t\_author@dc.mu.edu}
}

\keywords{Logo Detection, Logo Retrieval, Logo Dataset, Trademark Retrieval, Open Set Retrieval, Deep Learning.}

\abstract{
%%TODO write text here, 70-200 words
Current logo retrieval research focuses on closed set scenarios. We argue that the logo domain is too large for this strategy and requires an open set approach. To foster research in this direction, the large-scale Logos in the Wild dataset is collected and released to the public.
Searching for logos in image data has many applications, with judging the effectiveness of advertisement in sports event broadcasts being one example. Given a query sample in shape of a logo image, the task is to find all further occurrences of this logo in a database of images or videos. Currently, common logo retrieval approaches are unsuitable for this task because of their closed world assumption. To address this issue, an open set logo retrieval method is proposed in this work which allows search for previously unfamiliar logos only by one query sample. A two stage concept with an open set logo detection and comparison is proposed. Both modules are based on task specific \acp{CNN}. The novel Logos in the Wild dataset serves to train the modules with sufficient appropriate in-the-wild data. 
The proposed method extends the application field in comparison to closed set approaches and significant improvements over baseline methods derived from these state-of-the-art closed set approaches are shown. 
}

\onecolumn \maketitle \normalsize \vfill

\section{\uppercase{Introduction}}
\label{sec:introduction}
%\noindent- sport event broadcasts \\
%- \ac{CNN}-based \\
%- separation of workflow into detection and comparison stage, similar to person/face re-id/retrieval \\
%- given as query: search image, not a logo class id or logo name \\
%- extension of the all-in-one architecture known from closed set logo retrieval \\
%
%- there exists a wide variety of different kinds of logos as large-scale datasets, such as METU with more than ?? different logo classes, indicate \\
%
%- task: find all occurrences of a query logo including typical variations, such as color inversions
%
%contributions: \\
%  - detection of any logo, not only predefined ones from training data \\
%  - open set logo retrieval system with search image as query \\
%  - introduction of novel logo dataset \\
\noindent Automated search for logos is a desirable task in visual image analysis.
A key application is the effectiveness measurement of advertisements. Being able to find all logos that match a query, for example, a logo of a specific company, in images allows to assess the visual frequency and prominence of logos in TV broadcasts. Typically, these broadcasts are sports events where sponsorship and advertisement is very common. 
This requires a flexible system where the query can easily be defined and switched according to the current task. Especially, also previously unseen logos should be found if one query sample is available.
%In general, open set retrieval is understood as finding instances of the same class in a database given a single instance from this class which has never been seen before. Usually classes originate from the same category with logos being an example. Thus, prior knowledge about this category can be used to build appropriate retrieval methods. In most applications this prior knowledge is given in shape of training data.
This requirement excludes basically all current logo retrieval approaches because they make a closed-world assumption where all searched logos are known beforehand. Instead, this paper focuses on open set logo retrieval where only one sample image of a logo is available. 

Consequently, a novel processing strategy for logo retrieval based on a logo detector and a feature extractor is proposed as illustrated in figure~\ref{fig:pipeline}. Similar strategies are known from other open set retrieval tasks, such as face or person retrieval~\cite{bauml2010,herrmann2015b}. Both, the detector and the extractor are task specific \acp{CNN}. For detection, the Faster R-CNN framework~\cite{ren2015} is employed and the extractor is derived from classification networks for the ImageNet challenge~\cite{deng2009}.
%
\begin{figure}[t]
  %\vspace{-0.2cm}
  \centering
  \includegraphics[width=\linewidth]{img/outline.png}
  \caption{Proposed logo retrieval strategy.}
  \label{fig:pipeline}
\end{figure}

The necessity for open set logo retrieval becomes obvious when having a look at the diversity and amount of existing logos and brands\footnote{The term brand is used in this work as synonym for a single logo class. Thus, a brand might also refer to a product or company name if an according logo exists.}. The METU trademark dataset~\cite{tursun2017} contains, for example, over half a million different logos. Given this number, a closed set approach where all different logos are pretrained within the retrieval system is clearly inappropriate.
This is why our proposed feature extractor generates a discriminative logo descriptor, which generalizes to unseen logos, instead of a mere classification between previously known brands. The well-known high discriminative capabilities of \acp{CNN} allow to construct such a feature extractor.

One challenge for training a general purpose logo detector lies in appropriate training data. Many logo or trademark dataset~\cite{eakins1998,hoi2015,tursun2017} only contain the original logo graphic but no in-the-wild occurrences of these logos which are required for the target application. The need for annotated logo bounding boxes in the images limits the number of suitable datasets. Existing logo datasets~\cite{joly2009,kalantidis2011,romberg2011,letessier2012,bianco2015,su2016,bianco2017} with available bounding boxes are often restricted to a very small number of brands and mostly high quality images. Especially, occlusions, blur and variations within a logo type are only partially covered.
To address these shortcomings, a novel in-the-wild logo dataset is collected and made publicly available~\footnote{\url{url://to.come}}.

The contributions of this work are threefold:
\begin{itemize}
\item A novel open set logo detector which can detect previously unseen logos.
\item An open set logo retrieval system which needs only a single logo image as query.
\item The introduction of a novel large-scale in-the-wild logo dataset.
\end{itemize}

%contributions: \\
%  - detection of any logo, not only predefined ones from training data \\
%  - open set logo retrieval system with search image as query \\
%  - introduction of novel logo dataset \\



\section{\uppercase{Related Work}}
%\noindent- Logo Retrieval (\xie closed set approaches) \\
\noindent Current logo retrieval strategies are generally solving a closed set detection and classification problem. Eggert et.al. \cite{eggert2015} utilized \acp{CNN} to extract features from logos and determined their brand by classification with a set of \acp{SVM}. Fast R-CNN \cite{girshick2015} was used for the first time to retrieve logos from images by Iandola et al. \cite{iandola2015} and achieved superior results on the FlickrLogos-32 dataset \cite{romberg2011}. Furthermore, R-CNN, Fast R-CNN and Faster R-CNN were used in \cite{bao2016}, \cite{oliveira2016}, \cite{qi2017}. All these works use the same brands for training as for validation.

%- Other retrieval applications (\xeg face, person)
\subsection{Open Set Retrieval}
Retrieval scenarios in other domains are basically always considered open set, \xie samples from the currently searched class have never been seen before.
This is the case for general purpose image retrieval~\cite{sivic2003}, tattoo retrieval~\cite{manger2012} or for person retrieval in image or video data where face or appearance-based methods are common~\cite{bauml2010,weber2011,herrmann2015b}. The reason is that these in-the-wild scenarios offer usually a too large and impossible to capture variety of object classes. In case of persons, a class would be a person identity with billions of persons existing. Consequently, methods have to be designed and trained on a limited set of classes and have to generalize to previously unseen classes. We argue that this approach is also required for logo retrieval because of the vast amount of existing brands and according logos which cannot be captured in advance.
Typically, approaches targeting open set scenarios consist of an object detector and a feature extractor~\cite{??}. The detector localizes the objects of interest and the feature extractor creates a discriminative descriptor regarding the target classes which can than be compared to query samples.

% Detector Frameworks: Faster R-CNN~\cite{ren2015}, SSD~\cite{liu2016b}, Yolo~\cite{redmon2016}, ...
\subsection{Object Detector Frameworks}
Early detectors applied hand-crafted features, such as Haar-like features, combined with a classifier to detect objects in images \cite{viola2004}. 
Nowadays, deep learning methods surpass the traditional methods by a significant margin. In addition, they allow a certain level of object classification within the detector which is mostly used to simultaneously detect different object categories \cite{sermanet2013}. 
%The OverFeat framework uses sliding windows on multiple scales of the image and combines the \ac{CNN} features to detect objects and classify them. 
The YOLO detector \cite{redmon2015} introduces an end-to-end network for object detection and classification based on bounding box regressors for object localization. This concept is similarly applied by the Single Shot MultiBox Detector (SSD) \cite{liu2015}. 
Faster Region-Based Convolutional Neural Network (R-CNN) \cite{ren2015} introduces a region proposal network (RPN) to detect object candidates in the feature maps and classifies the candidate regions by a fully connected network. 
Improvements of the Faster R-CNN are the Region-based Fully Convolutional Network (R-FCN) \cite{jifengdai2016b}, which reduces inference time by an end-to-end fully convolutional network, and the Mask R-CNN \cite{he2017}, adding a classification mask for instance segmentation.

% Classification Networks: AlexNet, VGG, ResNet, DenseNet
\subsection{\acs{CNN}-based Classification}
AlexNet \cite{krizhevsky2012b} was the first neural network after the conquest of \acp{SVM}, achieving impressive performance on image content classification and winning the ImageNet challenge \cite{deng2009}. It consists of five convolutional layers, each followed by a max-pooling, which counted as a very deep network at the time. 
VGG \cite{simonyan2014} follows the general architecture of AlexNet with an increased number of convolutional layers achieving better performance. 
The inception architecture~\cite{szegedy2015} proposed a multi-path network module for better multi-scale addressing, but was shortly after superseded by the Residual Networks (ResNet) \cite{he2015,he2016}. They increase network depth heavily up to 1000 layers in the most extreme configurations by additional skip connections which bypass two convolutional layers. 
The recent DenseNet \cite{huang2016} builds on a ResNet-like architecture and introduces ``dense units''. The output of these units is connected with every subsequent dense unit's input by concatenation. This results in a much denser network than a conventional feed-forward network.


\section{\uppercase{Logo Detection}}
%\noindent- class agnostic detector \\
%- different base networks: VGG-CNN-M, VGG16
% TODO
\noindent The current state-of-the-art approach for scene retrieval is to create a global feature of the input image. This is achieved either by inferring from the complete image or by searching for key regions and then extracting features from the located regions, which are finally fused into a global feature. For logo retrieval, extraction of a global feature is counterproductive because it lacks discriminative power to retrieve small objects. Additionally, global features usually include no information about the size and location of the objects which is also an important factor for logo retrieval.

Therefore, we choose a two-stage approach consisting of logo detection and logo classification as illustrated in figure~\ref{fig:pipeline}. First, the logos have to be detected in the input image. 
There are a lot of options to search for objects as explained in cha. Girshick et al. \cite{ren2015} proposed the Faster R-CNN, for end to end learning to detect and classify objects on an image. This network has a bounding box regressor for each trained class, thus it is capable to produce object type specific region proposals.

% Logo comparison and classification can be included in the Faster R-CNN framework at the cost of generalization to unseen classes. We argue that specialization to a few dozen specific brands does not cover the complexity and breadth of the domain.

\section{\uppercase{Logo Comparison}}
%TODO write text here


\section{\uppercase{Logo Dataset}}
%\noindent- freely collected from web \\
%- name suggestions: 'Logos in the Wild', 'IOSB Logos' \\
%- how was the dataset collected? \\
%- database statistics \\
%- what is different compared with other datasets? \\
%- what are the benefits? \\
%%TODO Christian
\noindent To train the proposed logo detector and feature extractor, a novel logo dataset is collected to supplement publicly available logo datasets. A comparison to other public in-the-wild datasets with annotated bounding boxes is given in table~\ref{tab:logoDatasets}.
%
\begin{table*}[t]
\centering
\caption{Publicly available in-the-wild logo datasets in comparison with the novel dataset.}
\label{tab:logoDatasets}
\begin{small}
\begin{tabular}{cl|ccc}
& \multicolumn{1}{c|}{\textbf{dataset}} & \textbf{brands} & \textbf{logo images} & \textbf{RoIs} \bigstrut[b]\\
\hline
\multirow{8}{*}{\vertboxs{public}} & BelgaLogos \cite{joly2009,letessier2012} & 37 & 1,321 & 2,697 \bigstrut[t] \\
& FlickrBelgaLogos \cite{letessier2012} & 37 & 2,697 & 2,697 \\
& Flickr Logos 27 \cite{kalantidis2011} & 27 & 810 & 1,261 \\
& FlickrLogos-32 \cite{romberg2011} & 32 & 2,240 & 3,404 \\
& Logos-32plus \cite{bianco2015,bianco2017} & 32 & 7,830 & 12,300 \\
& TopLogo10 \cite{su2016} & 10 & 700 & 863 \bigstrut[b] \\
\cline{2-5}
& combined & 80 (union) & 15,598 & 23,222 \bigstrut \\
\hline
%\begin{minipage}[c]{0.1cm}\vertboxt{new}\end{minipage} & Logos in the Wild & 868 & 11,039 & 32,808 \bigstrut[t]
\begin{minipage}[c]{0.1cm}\vertboxt{new}\end{minipage} & Logos in the Wild & 872 & 11,054 & 32,850 \bigstrut[t]
\end{tabular}
\end{small}
\end{table*}
%
The goal is an in-the-wild logo dataset with pictures including logos instead of the pure original logo graphics. In addition, images where the logo does not represent the central dominant part of the image are preferred. See figure~\ref{fig:logoSamples} for a few examples of the collected data.
%
\setlength{\frameSize}{3.7cm}
\begin{figure*}%
\centering%
\includegraphics[height=\frameSize]{img/sample2.png}
\hfill
\includegraphics[height=\frameSize]{img/sample12.png}
\hfill
\includegraphics[height=\frameSize]{img/sample3.png}%
\\
\vspace{1.5mm}%
\includegraphics[height=\frameSize]{img/sample15.png}
\hfill
\includegraphics[height=\frameSize]{img/sample5.png}
\hfill
\includegraphics[height=\frameSize]{img/sample9.png}
\caption{Examples from the collected Logos in the Wild dataset.}%
\label{fig:logoSamples}
\end{figure*}%
%
Following the general suggestions from \cite{bansal2017}, we target for a dataset containing significantly more brands instead of collecting additional image samples for the already covered brands. This is the exact opposite strategy than performed by the Logos-32plus dataset.
Starting with a list of well-known brands and companies, an image web search is performed. Because most other web collected logo datasets mainly rely on Flickr, we opt for Google image search to broaden the domain. Brand or company names are searched directly or in combination with a predefined set of search terms, \xeg, `bmw advertisement', `bmw building', `bmw poster' or `bmw store'. 

For each search result, the first $N$ images are downloaded, where $N$ is determined by a quick manual inspection to avoid collecting too much garbage. 
%All search results are quickly inspected manually if a sufficient amount of suitable in-the-wild logo images is included and if so the first $N$ images are downloaded. 
After removing duplicates, this results in 4~to 608~images per searched brand. These images are then one-by-one annotated with logo bounding boxes or sorted out if unsuitable.
Images are considered unsuitable if they contain no logos or fail the in-the-wild requirement, which is the case for the original raw logo graphics. Taken pictures of such logos and advertisement posters on the other hand are desired to be in the dataset. 
Annotations distinguish between textual and graphical logos as well as different logos from one company as exemplary indicated in figure~\ref{fig:annotatedSample}.
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 9cm 0cm 0cm, clip]{img/annotatedSample.pdf}%
\caption{Annotations differentiate between textual and graphical logos.}%
\label{fig:annotatedSample}
\end{figure}%
%
Altogether, the current version of the dataset, which is used in this paper, contains 631 brands with 17,738 annotated bounding boxes. 150 brands occur at least 10 times. An image may contain several logos with the maximum being 100 logos in one image. The complete distributions are shown in figures~\ref{fig:brandDistribution} and~\ref{fig:logoDistribution}. 
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 11cm 0cm 0cm, clip]{img/brandDistribution.pdf}%
\caption{Distribution of number of RoIs per brand.}%
\label{fig:brandDistribution}
\end{figure}%
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 11cm 0cm 0cm, clip]{img/logoDistribution.pdf}%
\caption{Distribution of number of RoIs per image.}%
\label{fig:logoDistribution}
\end{figure}%

The collected Logos in the Wild dataset exceeds the size of all related logo datasets as shown in table~\ref{tab:logoDatasets}. Even the union of all related logo datasets contains significantly less brands and RoIs which makes Logos in the Wild a valuable large-scale dataset.
 %Table~\ref{tab:datasetStatistics} gives more details.
The annotation is still an ongoing process and further larger versions of the dataset are expected to be published in the future (??).
%
%Potential uses: 
%In this work, the dataset is mainly used for training purposes
%In addition to the simple use as training data, 


\section{\uppercase{Experiments}}
%\noindent- explain FROC curve \\
%- maybe introduce mAP \\
%- explain baseline method(s) \\
%concepts/desired experiments \\
%- train on public non-Flickr brands \\
%- additionally train on own logo dataset \\
%- test on FlickrLogos-32 test datasets \\
%
%- additional evaluation on own SportsLogos dataset and qualitative results \\
\noindent The proposed method is evaluated on the test set benchmark of the public FlickrLogos-32 dataset including the distractors. Additional application specific experiments are performed on an internal dataset of sports event TV broadcasts. 
The training set consists of two parts. The union of all public logo datasets as listed in table~\ref{tab:logoDatasets} and the novel \ac{LitW} dataset. For a proper separation of train and test data, all brands which are present in the FlickrLogos-32 test set are removed from the public and \ac{LitW} data. 10 percent of the remaining images are set aside for network validation in each case. This results in the final training and test set sizes listed in table~\ref{tab:trainTestStatistics}.
%
\begin{table}[t]
\centering
\begingroup	
\setlength{\tabcolsep}{6pt}
\caption{Train and test set statistics.}
\label{tab:trainTestStatistics}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'Tabelle1'
\begin{tabular}{cl|rr}
\textbf{phase} & \multicolumn{1}{c|}{\textbf{data}} & \multicolumn{1}{c}{\textbf{brands}} & \multicolumn{1}{c}{\textbf{RoIs}} \bigstrut[b]\\
\hline
\multirow{2}[2]{*}{train} & public & 47    & 3.113 \bigstrut[t]\\
      & public+\ac{LitW} & 632   & 18.960 \bigstrut[b]\\
\hline
test  & FlickrLogos-32 test & 32    & 1.602 \bigstrut[t]\\
\end{tabular}%
\end{small}
\endgroup
\end{table}

In the first step, the detector stage alone is assessed. Then, the combination of detection and comparison for logo retrieval is evaluated. 
Detection and matching performance is measured by the \ac{FROC} curve~\cite{miller1969} which denotes the detection or detection and identification rate versus the number of false detections.

In all cases the \acp{CNN} are trained until convergence which requires ?? to ??k iterations with a batch-size of ??. Training duration depends on the architecture as well as the amount of training data.
%TODO write text here

\subsection{Detection}
%- train 3 different detectors: multi-class Faster R-CNN (maybe RPN), single-class Faster R-CNN, single-class Faster R-CNN + own logos\\
%- test on FlickrLogos-32 => FROC plot\\
%- test first two detectors on own logo dataset => FROC plot\\
%- Example Detections\\
%%TODO write text here
As baseline, the state-of-the-art closed set logo retrieval method from~\cite{su2016} based on Faster R-CNN is employed and trained on the public portion of the training data.
It is adapted to open set detection by using the RPN scores as detections. This skips the closed set classification part of the network which is pre-trained on different logos than should be detected on the test set.
%Because of the closed set training strategy, only brands occurring in the training set are the final outcome of this detector. 
%Adaptation to open set detection of previously unseen logos is achieved by using the RPN scores before classification as detections.
%Because of the strict separation of train and test brands, and the closed set training strategy
%the RPN scores serve as detections.
The proposed logo detector is first trained on the same public data for comparison. The results in figure~\ref{fig:detectionFroc} indicate that this strategy is superior by a significant margin. 
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 9cm 0cm 0cm, clip]{img/detectionFroc.pdf}%
\caption{Detection FROC curves for FlickrLogos-32 test set.}%
\label{fig:detectionFroc}
\end{figure}%
%

Further improvement is achieved by combining the public training data with the novel data.
Adding \ac{LitW} as additional training data improves the detection results with its large variety of additional training brands. 
This confirms findings from other domains, such as face analysis, where wider training datasets are preferred over deeper ones~\cite{bansal2017}. This means it is better to train on additional different brands than on additional samples per brand.
As direction for future dataset collection, this suggests to focus on additional brands.

\subsection{Retrieval}
%- train 4 to 6 additional networks: feature extraction network for public and public+own training data --- use VGG16, ResNet50, DenseNet as base network (either for both training sets or only for the public+own)
%- describe repeated query logo strategy => 10 sample => mean + std
%- results on FlickrLogos-32 => FROC plot\\
%- results on own sports data => FROC plot\\
%%TODO write text here
%
For the retrieval experiments, the state-of-the-art closed set logo retrieval method from the previous section is again used as baseline. The class probabilities are interpreted as feature vector which is then used to match previously unseen logos.
For the proposed open set strategy, the best logo detection network from the previous section is used in all cases. Detected logos are described by the classification network's output feature. Descriptor matching is performed in all cases with cosine similarity. Three different state-of-the-art classification architectures, namely VGG16~\cite{simonyan2014}, ResNet101~\cite{he2015} and DenseNet161~\cite{huang2016}, serve as base for the logo classification stage. All networks are pretrained on ImageNet and afterwards fine-tuned either on the public logo train set or the combination of the public and the \ac{LitW} train data.

\subsubsection*{FlickrLogos-32}
\vspace*{-2.5mm}
In 10 iterations, each of the 10 FlickrLogos-32 train samples for each brand serves as query sample. This allows to assess the statistical significance of results similar to a 10-fold-cross-validation strategy. Figure~\ref{fig:classificationFroc} shows the FROC results for the trained networks including indicators for the standard deviation of the measurements. The detection identification rate denotes the amount of ground truth logos which are correctly detected and are assigned the correct brand.
While the baseline is only able to find a minor amount of the logos, our best performing approach is able to correctly retrieve 25 percent of the logos if tolerating only one false alarm every 100 images.
As expected, the more recent network architectures provide better results. Also, including the \ac{LitW} data in the training yields a significant boost in performance. Specifically, the larger training dataset has a larger impact on the performance than a better network architecture. 
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 7cm 0cm 0cm, clip]{img/classificationFroc.pdf}%
\caption{Detection+Classification FROC curves for FlickrLogos-32 test set. Including dashed indicators for one standard deviation. DenseNet results are omitted for clarity, refer to table~\ref{tab:mapFlickr} for full results.}%
\label{fig:classificationFroc}
\end{figure}%
%
%As far as we are aware, this is the first experiment with open set logo retrieval on 
Table~\ref{tab:mapFlickr} compares our open set results with closed set results from the literature in terms of the mean average precision (\map). 
%
\begin{table}[t]
\centering
\begingroup	
\setlength{\tabcolsep}{6pt}
\caption{FlickrLogos-32 test set retrieval results.}
\label{tab:mapFlickr}
\begin{small}
% Table generated by Excel2LaTeX from sheet 'FlickrLogos-32Test'
\begin{tabular}{crc}
\textbf{setting} & \multicolumn{1}{c}{\textbf{method}} & \textbf{\map} \bigstrut[b]\\
\hline
\multirow{7}[2]{*}{\begin{sideways}open set\end{sideways}} & baseline, public~\cite{su2016} & 0.036 \bigstrut[t]\\
      & VGG16, public & 0.286 \\
      & ResNet101, public & 0.327 \\
      & DenseNet161, public & 0.368 \\
\cline{2-3}      
      & VGG16, public+\ac{LitW} & 0.382 \bigstrut[t]\\
      & ResNet101, public+\ac{LitW} & \textbf{0.464} \\
      & DenseNet161, public+\ac{LitW} & 0.448  \bigstrut[b]\\      
\hline
\multirow{4}[1]{*}{\begin{sideways}closed set\end{sideways}} & BD-FRCN-M \cite{oliveira2016} & 0.735 \bigstrut[t]\\	
      & DeepLogo \cite{iandola2015} & 0.744 \\ 
      & Faster-RCNN \cite{su2016} & 0.811 \\
      & Fast-M \cite{bao2016} & \textbf{0.842} \\
\end{tabular}%
\end{small}
\endgroup
\end{table}
We achieve more than half of the closed set performance in terms of \map with only one sample for a brand at test time instead of dozens or hundreds of brand samples at training time. In addition, our approach is not limited to the 32 FlickrLogos brands but generalizes with a similar performance to further brands. In contrast, the closed-set approaches hardly generalize as is shown by the baseline open set method. This is the second best closed set approach only retrained on out-of-test brands.

\subsubsection*{SportsLogos}
\vspace*{-2.5mm}
In addition to public data, target domain specific experiments are performed on TV broadcasts of sports events. In total, 1,218 annotated frames with more than 10,000 logos from four different events are available in our SportsLogos dataset where 3 events are used for training and one as test set. Refer to table~\ref{tab:sportDataStatistics} for details. In comparison to public logo datasets, the logos are usually significantly smaller in these cases and cover only a tiny fraction of the image area as illustrated in figure~\ref{fig:footballSample}. Consequently, the results in this real world scenario as indicated in figure~\ref{fig:classificationFrocFootball} are slightly worse than in the FlickrLogos-32 benchmark. Nevertheless, training with \ac{LitW} data again improves the results significantly.
%
\begin{figure*}%
\centering%
\includegraphics[width=\linewidth]{img/football-1908719_1920_cut.jpg}%
\caption{Example football scene with small logos in the perimeter advertising.}%
\label{fig:footballSample}
\end{figure*}%
%
%Due to copyright reasons this internal dataset cannot be made publicly available.
%
\begin{table}[t]
\centering
\begingroup	
\setlength{\tabcolsep}{5pt}
\caption{SportsLogos dataset statistics.}
\label{tab:sportDataStatistics}
\begin{small}
\begin{tabular}{l|cccc}
 & \textbf{phase} & \textbf{brands} & \sizebox{1.0cm}{\textbf{logo images}} & \textbf{RoIs} \bigstrut[b]\\
\hline
football-1 & \multirow{3}{*}{train} & {104} & {331} & {3,329} \bigstrut[t]\\
{ski} & & {27} & {179} & {701} \\
{ice hockey} & & {19} & {410} & {3,920} \bigstrut[b]\\
\hline
{football-2} & test & 40 & 298 & 2,348 \bigstrut\\
%\hline
%{total} & & {143 (union)} & {1,218} & {10,298} \bigstrut[t]\\
\end{tabular}
\end{small}
\endgroup
\end{table}
%
\begin{figure}%
\centering%
\includegraphics[width=\linewidth, trim=0cm 7cm 0cm 0cm, clip]{img/classificationFrocFootball.pdf}%
\caption{Detection+Classification FROC curve for SportsLogos test set. \map is given in brackets.}%
\label{fig:classificationFrocFootball}
\end{figure}%
%

\section{\uppercase{Conclusions}}
\label{sec:conclusion}
\noindent- significant improvement over baseline
- enables novel applications
- novel large scale in-the-wild logo dataset
%TODO write text here


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vfill
\bibliographystyle{apalike}
{\small
\bibliography{Biblio}}

\vfill
\end{document}

\\
